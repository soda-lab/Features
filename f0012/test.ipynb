{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "ip = config['DEFAULT']['IP']\n",
    "port = config['DEFAULT']['MongoDB-Port']\n",
    "db_name = config['DEFAULT']['DB-Name']\n",
    "contain_string = config['DEFAULT']['Contain-String']\n",
    "output_path = config['DEFAULT']['Output-Path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(ip, int(port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Collection Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "db = client[db_name]\n",
    "collections_twitter = db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_collection = {}\n",
    "for i in collections_twitter:\n",
    "    if contain_string in i:\n",
    "        dic_collection[i] = \"{:}\".format(db[i].find({}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for aggregation\n",
    "pipeline = [\n",
    "    {\"$match\": { \"entities.hashtags\": {\"$exists\":True,\"$ne\":[]}}},\n",
    "    {\"$match\": { \"lang\" : \"en\"}},\n",
    "    { \"$group\": {\n",
    "        \"_id\": {\n",
    "            \"hashtags\": \"$entities.hashtags\",\n",
    "            \"date\": {\"$substr\": [ \"$created_at\", 4, 6 ]},\n",
    "            \"geoname\": \"$geoname\"\n",
    "        },\n",
    "        \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if stringis English\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create foler if not exist\n",
    "def create_folder(output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existed collection from the list dic_collection\n",
    "def delete_collection(output_path,dic_collection):\n",
    "    for input_file in glob.glob(os.path.join(output_path,'*.csv')):\n",
    "        collection_name = re.search(output_path+'(.+?).csv', input_file).group(1)\n",
    "        if collection_name in dic_collection:\n",
    "            print(\"Existed collection: \" + collection_name)\n",
    "            del dic_collection[collection_name]\n",
    "    return dic_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exist(data_format,h,date_year,city,state,country,exist,dic,count):\n",
    "    for d in data_format:\n",
    "        if (h in d[\"hashtag\"]) and (date_year in d[\"date\"]) and (city in d[\"city\"]) and (state in d[\"state\"]) \\\n",
    "        and (country in d[\"country\"]):\n",
    "            d[\"count\"] += count     \n",
    "            exist = 1\n",
    "    if exist == 0:\n",
    "        data_format.append(dic)\n",
    "    return data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count hashtag daily for each collection\n",
    "def count_hashtag(length,hashtags,date_year,city,state,country,count,data_format,num_delete):\n",
    "    for i in range(0,length):\n",
    "        exist = 0\n",
    "        # get hashtag\n",
    "        h = hashtags[i][\"text\"].lower()\n",
    "        # check if it is in English  \n",
    "        if isEnglish(h):\n",
    "            dic = {\"hashtag\": h, \"date\":date_year, \"count\":count, \"city\":city, \"state\":state, \"country\":country}\n",
    "            if len(data_format) > 0 :\n",
    "                #check if the record exists\n",
    "                data_format = check_exist(data_format,h,date_year,city,state,country,exist,dic,count)\n",
    "            else:        \n",
    "                data_format.append(dic)\n",
    "        else:\n",
    "            num_delete += 1\n",
    "    return data_format, num_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data list\n",
    "def create_list(data_list,data_format,year):\n",
    "    num_delete = 0\n",
    "    for data in data_list:\n",
    "        hashtags = data[\"_id\"][\"hashtags\"]\n",
    "        date_year = data[\"_id\"][\"date\"] + \" \" + year\n",
    "        if \"geoname\" in data[\"_id\"]:\n",
    "            city = data[\"_id\"][\"geoname\"][\"city\"]\n",
    "            state = data[\"_id\"][\"geoname\"][\"state\"]\n",
    "            country = data[\"_id\"][\"geoname\"][\"country\"]\n",
    "        else:\n",
    "            city = ''\n",
    "            state = ''\n",
    "            country = ''\n",
    "            \n",
    "        count = data[\"count\"]        \n",
    "        length = len(hashtags)\n",
    "        \n",
    "        data_format,num_delete = count_hashtag(length,hashtags,date_year,city,state,country,count,data_format,num_delete)\n",
    "    return data_format,num_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv file\n",
    "def write_csv(collection,data_format,output_path,num_delete,total_tweet_count):\n",
    "    file_name = output_path + collection + \".csv\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        # header\n",
    "        f.write('hashtag,date,hashtag_count,city,state,country,non_english_hashtag_count,total_tweet_count\\n')\n",
    "        for data in data_format:\n",
    "            line = \"{},{},{},{},{},{},{},{}\\n\".format(data['hashtag'],data['date'],data[\"count\"],data[\"city\"] \\\n",
    "                                                      ,data[\"state\"],data[\"country\"],num_delete,total_tweet_count)\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of hashtag daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "\n",
      "Processing on collection: 2018_W52_Twitter_Australia\n",
      "hashtag and date list is finished\n",
      "194 non-English hashtags have been deleted.\n",
      "hashtag csv file for collection 2018_W52_Twitter_Australia is finished.\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create folder if not exist\n",
    "create_folder(output_path)\n",
    "\n",
    "# delete existed collection from the list dic_collection\n",
    "dic_collection = delete_collection(output_path,dic_collection)\n",
    "\n",
    "for collection in sorted(dic_collection):\n",
    "    print(\"-----------------------\\n\")\n",
    "    print(\"Processing on collection: \" + collection)\n",
    "    \n",
    "    data_format = []\n",
    "    num_delete = 0\n",
    "    #total_tweet_count = dic_collection[collection]\n",
    "    total_tweet_count = 111\n",
    "    data_list = list(db[collection].aggregate(pipeline,allowDiskUse=True))\n",
    "    year = collection[:4]\n",
    "    \n",
    "    if len(data_list) > 0:\n",
    "        data_format, num_delete = create_list(data_list,data_format,year)\n",
    "                    \n",
    "    print(\"hashtag and date list is finished\")\n",
    "    print(str(num_delete) + \" non-English hashtags have been deleted.\")\n",
    "    \n",
    "    write_csv(collection,data_format,output_path,num_delete,total_tweet_count)\n",
    "\n",
    "    print (\"hashtag csv file for collection \" + collection + ' is finished.')\n",
    "    print(\"-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
